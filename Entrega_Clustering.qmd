---
title: "Clustering"
author: "Xisco Ribera & Irene Julià"
format:
  html:
    toc: true
    toc-depth: 5
editor: visual
---

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(dplyr)
library(GGally)
library(psych)
library(skimr)
library(fmsb)
library(mvnormtest)
library(car)
library(nortest)
library(fBasics)
library(ggplot2)
library(factoextra)
library(stats)
library(cluster)
library(NbClust)
library(ggrepel)
```

# Recordatorio

Vamos a tratar una base de datos relacionados con la predicción de riesgo de cirrosis. La cirrosis es una etapa tardía de la cicatrización (fibrosis) del hígado causada por muchas formas de enfermedades y afecciones hepáticas, como la hepatitis y el alcoholismo crónico.

Nuestro objetivo para este estudio sería estudiar algunos perfiles con riesgo de padecer cirrosis.

Los datos provienen de la página web Kaggle: [Kaggle - Cirrhosis](https://www.kaggle.com/datasets/fedesoriano/cirrhosis-prediction-dataset/).

Recordemos nuestra tabla de datos:

```{r}
datos <- read.table("cirrosis_tidy.csv",  header = TRUE )
datos = datos %>% mutate(Ascites = Ascites %>% as.factor,
                         Hepatomegaly = Hepatomegaly %>% as.factor,
                         Spiders = Spiders %>% as.factor,
                         Edema = Edema %>% as.factor,
                         Stage = Stage %>% as.factor,
                         Drug = Drug %>% as.factor,
                         Sex = Sex %>% as.factor,
                         Status = Status %>% as.factor)


glimpse(datos)
```

El tibble resultante consta de 276 observaciones y 20 variables. Cada muestra representa un paciente al que se le ha extraido la siguiente información:

-   `ID`: Identificador único
-   `N_Days`: Número de días entre el registro y la fecha de defunción, transplante o estudio analítico en Julio de 1986.
-   `Status`: Estatus del paciente: C (Censurado), CL (censurado debido a tratamiento hepático), o D (Muerto)
-   `Drug`: Tipo de fármaco: D-penicilamina o placebo
-   `Age`: Edad \[días\]
-   `Sex`: Sexo cromosómico: Male (hombre) o Female (Mujer)
-   `Ascites`: Presencia de Ascitis: No o Si
-   `Hepatomegaly`: Presencia de Hepatomegalia: No o Si
-   `Spiders`: Presencia de arañas vasculares: No o Si
-   `Edema`: Presencia de Edema: No (no hay edema y sin tratamiento diurético para el edema), Sin (presencia de edema sin diuréticos, o edema curado con diuréticos), o Si (edema a pesar del tratamiento con diuréticos)
-   `Bilirubin`: Bilirrubina sérica \[mg/dl\]
-   `Cholesterol`: Colesterol sérico \[mg/dl\]
-   `Albumin`: Albúmina \[g/dl\]
-   `Copper`: Cobre en orina \[ug/day\]
-   `Alk_Phos`: Fosfatasa alcalina \[U/liter\]
-   `SGOT`: SGOT \[U/ml\]
-   `Triglycerides`: Triglicéridos \[mg/dl\]
-   `Platelets`: Plaquetas por centímetro cúbico \[ml/1000\]
-   `Prothrombin`: Tiempo de Protrombina \[s\]
-   `Stage`: Estado histórico de la enfermedad (1, 2, 3, or 4)

## Resumen numérico de las variables

-   Datos cuantitativos:

```{r, echo=FALSE}
# Separamos los datos en variables cuantitativas y cualitativas
datos_quant <- datos %>% 
  select(where(is.numeric)) %>% 
  select(-1)


datos_qual <- datos %>% 
  select(where(is.factor))


# Cuantitativas

Unidad = c("Días", "Días", "mg/dl", "mg/dl", "g/dl", "ug/día", "U/l", "U/ml", "mg/dl", "ml/1000", "s")

Media = round(colMeans(datos_quant),3)

rango <- function(x){
  return(max(x)-min(x))
}
Rango = round(apply(datos_quant, FUN = rango, MARGIN = 2), 3)
Minimo = round(apply(datos_quant, FUN = min, 2),3)
Maximo = round(apply(datos_quant, FUN = max, 2),3)
Desv = round(apply(datos_quant, FUN = sd, 2), 3)

tabla = data.frame(Unidad, Media, Desv, Minimo, Maximo, Rango)

tabla
```

-   Datos cualitativos:

```{r, echo=FALSE}
summary(datos_qual)
```

## Análisis de normalidad multivariante

Con estos datos vamos a realizar nuestro estudio de normalidad multivariante.

Calculemos el vector de medias

```{r, echo = FALSE}
Medias = colMeans(datos_quant) # vector de medias

S = cov(datos_quant) # matriz de covarianza
```

y la distancia de Mahalanobis:

```{r}
d_Mahalanobis = apply(datos_quant, MARGIN = 1, function(x)
                    t(x - Medias)%*%solve(S)%*%(x - Medias))
```

Una vez calculadas estas medidas, representemos los datos

```{r, echo=FALSE}
plot(qchisq((1:nrow(datos_quant) - 1/2) / nrow(datos_quant), df = 3), sort(d_Mahalanobis), xlab = expression(paste("Cuantiles de la ", chi[20]^2)),ylab="Distancias ordenadas")
abline(a=0,b=1)
```

Notemos que no sigue una Chi-cuadrado, i por tanto los datos tampoco siguen una normal multivariante.

Vamos a realizar un test de normalidad para confirmarlo. Utilizaremos Shapiro-Wilk:

```{r}
mvnormtest::mshapiro.test(t(datos_quant))
```

Obtenemos un p-valor muy pequeño, prácticamente 0, entonces, rechazamos la hipótesis nula y concluimos que no hay normalidad multivariante, es decir, almenos una variable individual no se distribuye normalmente.

# Clustering

Vamos a guardar en un nuevo dataset las variables cuantitativas. Vamos a tipificar o escalar nuestros datos para que esten todos a la misma escala:

```{r}
datos2 <- datos_quant %>% scale()
```

A continuación, vamos a centrar los datos:

```{r}
n <- dim(datos2)[1]
X <- as.matrix(datos2)
Hn <- diag(n)-1/n # matriz de centrado
cX <- Hn%*%X # matriz centrada
```


Vamos a representar la matriz de distancia, utilizando distancia euclidea.

```{r}
mat_dist <- dist(x = cX, method = "euclidean")
```

```{r, cache = TRUE}
fviz_dist(dist.obj = mat_dist, lab_size = 5) +
 theme(legend.position = "none")
```

## K-means

En nuestro caso, no sabemos en cuantos clusters o grupos esta dividido nuestro dataset. Por tanto, vamos a estimar al número $k$ óptimo para aplicar el método de *k-means*. Para ello, utilizaremos la función `fviz_nbclust()`:

```{r}
fviz_nbclust(x = cX, FUNcluster = kmeans, method = "wss",
 diss = dist(cX, method = "euclidean"))+
  geom_vline(xintercept = 4, linetype = 2)
```
El método del codo nos sugiere considerar 4 clusters, aunque no sea demasiado clara la decisión. De todas formas, otro motivo para considerar 4 es para poder comparar con la variable `Stage` que tiene 4 niveles.

Calculamos los 4 centroides:

```{r}
set.seed(232)
km_clusters_4 <- kmeans(x = cX, centers = 4, nstart = 70)
```


Ahora bien, representemos dichos clústers en el plano. Como nuestro número de variables (dimensionalidad) es mayor de 2, automáticamente realiza un PCA y representa las dos primeras componentes principales (Dim1 y Dim2).

```{r}
fviz_cluster(object = km_clusters_4, data = cX, show.clust.cent = TRUE, geom ="point",
 ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, palette = c("#FF7078", "#F39B4C", "#7FBFF5", "#A298E8")) +
theme_bw() +
theme(legend.position = "right")
```

Como podemos ver, al menos en la proyección en $2$ dimensiones, hay bastante solapamiento. Además, si vemos la zona pintada como si fuera un intervalo de confianza, hay muchas observaciones que quedan fuera.


En este punto, sería interesante ver si los clusters corresponden a las fases de cirrosis segun la variable `stage`.

Vamos a crear un data frame compuesto de $3$ columnas: identificador del paciente, estado de la enfermedad y cluster al que pertenece, y observemos una tabla de frecuencias absolutas de cada fase de enfermedad y cada cluster:

```{r}
id_stage = datos %>% 
  select(ID, Stage)
Cluster = km_clusters_4$cluster %>% as.factor()

tabla_cluster = cbind(id_stage, Cluster)

table(x = tabla_cluster[,c(2,3)])
```

Observemos que, en este caso, un kmeans con $k=4$ no clusteriza en función del estado de cirrosis en que se encuentra el paciente.

## K-medoids (PAM)

En este caso, cada cluster está representado por una observación presente en el cluster (medoid), mientras que en *K-means* cada cluster está representado por su centroide, que se corresponde con el promedio de todas las observaciones del cluster pero con ninguna en particular. Vamos a utilizar la distancia de Manhattaan ya que es menos sensible a outliers.

```{r, warning=FALSE}
fviz_nbclust(x = cX, FUNcluster = pam, method = "wss",
 diss = dist(datos, method = "manhattan"))
```

Encontramos codos en $k=2$, $k=4$ y uno muy pronunciado en $k=6$. Para ver cual sería óptimo, visualicemos otros métodos como el de la *silueta* y *gap_stat*.

```{r, warning=FALSE}
fviz_nbclust(x = cX, FUNcluster = pam, method = "silhouette",
 diss = dist(datos, method = "manhattan"))
```

```{r, warning=FALSE}
fviz_nbclust(x = cX, FUNcluster = pam, method = "gap_stat",
 diss = dist(datos, method = "manhattan"))
```

Estos dos métodos nos sugieren $2$ clusters para realizar el análisis. En nuestro contexto, tiene más sentido utilizar 2, así que calculamos los medoides:

```{r}
pam_clusters <- pam(x = cX, k = 2, metric = "manhattan")

fviz_cluster(object = pam_clusters, data = cX, ellipse.type = "t", geom = "point", repel =TRUE) +
  theme_bw() + 
  theme(legend.position = "right")
```

Estan bastante solapados, puede deberse a la proyección a dos dimensiones.

Comparemos los clusters que se han realizado con la variable `sex`:

```{r}
id_sex = datos %>% 
  select(ID, Sex)

Cluster_medoid_2 = pam_clusters$clustering %>% as.factor()

tabla_cluster = cbind(id_sex, Cluster_medoid_2)

table(x = tabla_cluster[,c(2,3)])
```
Observemos que, en este caso, un *k-medoids* con $k=2$ no clusteriza en función del sexo biológico del paciente.

Hagamos lo mismo con las variables `Drug` y `Hepatomegaly`

```{r}
id_drug = datos %>% 
  select(ID, Drug)

Cluster_medoid_2 = pam_clusters$clustering %>% as.factor()

tabla_cluster = cbind(id_drug, Cluster_medoid_2)

table(x = tabla_cluster[,c(2,3)])
```

```{r}
id_hepatomegaly = datos %>% 
  select(ID, Hepatomegaly)

Cluster_medoid_2 = pam_clusters$clustering %>% as.factor()

tabla_cluster = cbind(id_hepatomegaly, Cluster_medoid_2)

table(x = tabla_cluster[,c(2,3)])
```

Observamos que tampoco clusteriza en función de `Drug` y `Hepatomegaly`.

Realicemos *k-medoids* con $k=4$ para comparar con la variable `stage`:

```{r}
pam_clusters_4 <- pam(x = cX, k = 4, metric = "manhattan")

fviz_cluster(object = pam_clusters_4, data = cX, ellipse.type = "t", geom = "point", repel =TRUE) +
  theme_bw() + 
  theme(legend.position = "right")
```
```{r}
id_stage = datos %>% 
  select(ID, Stage)

Cluster_medoid = pam_clusters_4$clustering %>% as.factor()

tabla_cluster = cbind(id_stage, Cluster_medoid)

table(x = tabla_cluster[,c(2,3)])
```

Observemos que, en este caso, un *k-medoids* con $k=4$ tampoco clusteriza en función del estado de cirrosis en que se encuentra el paciente.


## Clara

Este método combina los dos anteriores. dividiendo a partes iguales el conjunto de observaciones y aplicando PAM a cada una de las partes para identificar los *medoids*. Calcula la suma total de las distancias entre cada observación y su correspondiente medoid y seleccionaa como clustering final aquel que ha conseguido menor suma total de distancias intra-clusters. 

Como en los casos anteirores han salido que $k=2, 4$ eran los óptimos, realicemos *Clara* con estos valores de $k$:

### $k=2$
```{r}
clara_clusters_2 <- clara(x = cX, k = 2, metric = "manhattan", stand = TRUE,
 samples = 50, pamLike = TRUE)

fviz_cluster(object = clara_clusters_2, ellipse.type = "t", geom = "point") +
theme_bw() +
theme(legend.position = "right")
```


Comparemos los clusters que se han realizado con la variable `sex`:

```{r}
id_sex = datos %>% 
  select(ID, Sex)

Cluster_clara_2 = clara_clusters_2$clustering %>% as.factor()

tabla_cluster = cbind(id_sex, Cluster_clara_2)

table(x = tabla_cluster[,c(2,3)])
```
Observemos que, en este caso, el método *Clara* con $k=2$ no agrupa en función del sexo biológico del paciente.

Hagamos lo mismo con las variables `Drug` y `Hepatomegaly`

```{r}
id_drug = datos %>% 
  select(ID, Drug)

Cluster_clara_2 = clara_clusters_2$clustering %>% as.factor()

tabla_cluster = cbind(id_drug, Cluster_clara_2)

table(x = tabla_cluster[,c(2,3)])
```

```{r}
id_hepatomegaly = datos %>% 
  select(ID, Hepatomegaly)

Cluster_clara_2 = clara_clusters_2$clustering %>% as.factor()

tabla_cluster = cbind(id_hepatomegaly, Cluster_clara_2)

table(x = tabla_cluster[,c(2,3)])
```

Observamos que *Clara* tampoco clusteriza en función de `Drug` y `Hepatomegaly`.


### $k=4$
```{r}
clara_clusters_4 <- clara(x = cX, k = 4, metric = "manhattan", stand = TRUE,
 samples = 50, pamLike = TRUE)

fviz_cluster(object = clara_clusters_4, ellipse.type = "t", geom = "point") +
theme_bw() +
theme(legend.position = "right")

```

Observemos que los resultados son bastante parecidos que en *k-means* y *k-medoids*. 

Comprobemos si la clusterización en $4$ grupos es semejante a la de la variable `stage`

```{r}
id_stage = datos %>% 
  select(ID, Stage)

Cluster_clara = clara_clusters_4$clustering %>% as.factor()

tabla_cluster = cbind(id_stage, Cluster_clara)

table(x = tabla_cluster[,c(2,3)])
```
Observemos que el método *Clara* con $k=4$ tampoco clusteriza en función de la fase de cirrosis en que se encuentra el paciente.


## Dendograma

Por útimo, realicemos un dendograma para ver como se van formando los clusters, con el algoritmo *agglomerative hierarchical clustering*. 

Como sabemos, existen diferentes tipos de dendogramas en función del tipo de distancia entre cluster que se escoja. Recordemos cuales son:

- **Complete or Maximum:** Se calcula la distancia entre todos los posibles pares formados por una observación del cluster A y una del cluster B. La mayor de todas ellas se selecciona como la distancia entre los dos clusters.

- **Single or Minimum:** Se calcula la distancia entre todos los posibles pares formados por una observación del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters.

- **Average:** Se calcula la distancia entre todos los posibles pares formados por una observación del cluster A y una del cluster B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters.

- **Centroid:** Se calcula el centroide de cada uno de los clusters y se selecciona la distancia entre ellos como la distancia entre los dos clusters.

- **Ward:** Se trata de un método general. La selección del par de clusters que se combinan en cada paso del *agglomerative hierarchical clustering* se basa en el valor óptimo de una función objetivo, pudiendo ser esta última cualquier función definida por el analista. Un caso particular es *Ward's minimum variance*, en cada paso se identifican aquellos 2 clusters cuya fusión conlleva menor incremento de la varianza total intra-cluster.


- Dendograma para $4$ clusters con distancia entre clusters *Complete or Maximum*:

```{r, cache=TRUE, warning=FALSE}
hc_completo <- mat_dist %>%
  hclust(method = "complete") # Complete or Maximum

fviz_dend(x = hc_completo, k = 4, cex = 0.6) +
  geom_hline(yintercept = 11.2, linetype = "dashed") +
  geom_hline(yintercept = 9.6, linetype = "dashed")
```

- Dendograma para $4$ clusters con distancia entre clusters *Single or Minimum*:


```{r, cache=TRUE, warning=FALSE}
hc_single <- mat_dist %>%
  hclust(method = "single") # Single or Minimum

fviz_dend(x = hc_single, k = 4, cex = 0.6) +
 geom_hline(yintercept = 5.9, linetype = "dashed")+
  geom_hline(yintercept = 5.1, linetype = "dashed")
```

- Dendograma para $4$ clusters con distancia entre clusters *Average*:

```{r, cache=TRUE, warning=FALSE}
hc_average <- mat_dist %>%
  hclust(method = "average") # Average

fviz_dend(x = hc_average, k = 4, cex = 0.6) +
 geom_hline(yintercept = 8.8, linetype = "dashed")+
  geom_hline(yintercept = 7, linetype = "dashed")
```

- Dendograma para $4$ clusters con distancia entre clusters *Ward's minimum variance*:

```{r, cache=TRUE, warning=FALSE}
hc_ward_D2 <- mat_dist %>%
  hclust(method = "ward.D2") # Ward's minimum variance

fviz_dend(x = hc_ward_D2, k = 4, cex = 0.6) +
 geom_hline(yintercept = 25, linetype = "dashed")+
  geom_hline(yintercept = 18, linetype = "dashed")
```

- Observemos que, al tener muchas observaciones, no podemos distinguirlas. Aun así nos podemos hacer una idea del tamaño de cada cluster. 

- Observamos que, en nuestros datos, los clusters que se forman dependen mucho de la distancia entre clusters que se escoja. Esto puede indicar que los datos no esten distribuidos en grupos bien diferenciados.

- Observamos que si cortamos más arriba el dendograma (la línea más alta), podemos ver cuales son los clusters que se juntan si quisiéramos solo $2$ clusters.


Una vez calculados los dendogramas, hay que comprobar que la estructura preserva bien las distancias originales entre las observaciones. Para hacerlo usemos el coeficiente de correlación entre las distancias *cophenetic* del dendrograma y la matriz de distancias original. Cuanto más cercano a 1 sea el valor, mejor refleja el dendograma las distancias originales.


```{r, results='hold'}
cor(x = mat_dist, cophenetic(hc_completo))
cor(x = mat_dist, cophenetic(hc_single))
cor(x = mat_dist, cophenetic(hc_average))
cor(x = mat_dist, cophenetic(hc_ward_D2))
```

- Observamos que las distancias *Single or Minimum* y *Average* preservan mejor las distancias entre observaciones, en cambio *Ward's minimum variance* en nuestro caso no es recomendable usarla.


# Análisis de Componentes Principales

Calculamos las componentes principales con el comando `prcomp` utilizando nuestro dataset, escalando los datos y centrados.

```{r}
datos.acp=prcomp(datos_quant, scale = TRUE, center = TRUE)
```

Los valores propios muestran el porcentaje de varianza explicada por cada componente principal.

```{r, echo=FALSE}
lambdas = get_eigenvalue(datos.acp)
round(lambdas, 5)
```

```{r, echo=FALSE}
fviz_eig(datos.acp, addlabels = TRUE, ylim=c(0,100))
```

Efectivamente, viendo el gráfico y utilizando el criterio del codo, nos quedaremos con $2$ o $3$ componentes principales. Pero como trabajamos junto a clusters, y estos están representados sobre 2 CPs, consideraremos solo las dos primeras componentes principales.

Realicemos un gráfico de círculo de correlación variable para ver como se agrupan las variables y la calidad de representación que tienen.

```{r, echo=FALSE}
fviz_pca_var(datos.acp, axes = c(1,2), col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)
```

Para corroborar numéricamente la calidad de representación, realizaremos un gráfico de cos2.

```{r, echo=FALSE}
var <- get_pca_var(datos.acp)
fviz_cos2(datos.acp, choice = "var", axes = 1:2)
```

Efectivamente, las dos primeras variables tienen un cos2 alto, por tanto estas bien representadas. En cambio, las otras variables tienen un valor de cos2 más bajo, indicando que estas variables no están tan bien representadas por las componentes principales.

Para ver como se relacionan las componentes principales con los datos originales, veamos los autovectores.

```{r, echo=FALSE}
round(datos.acp$rotation[,c(1,2)],4)
```

Observemos que la primera componente principal da un peso muy elevado y negativo a la variable `Bilirubin`.

Respecto a la segunda componente principal, representa sobretodo a `Platelets`,  a `Age` y a `Cholesterol`.

Ahora, vamos a realizar un `biplot`, que nos permitirá visualizar las variables originales y las observaciones transformadas en los ejes de componentes principales.

```{r, echo=FALSE}
fviz_pca_biplot(datos.acp,  repel = TRUE,
                col.var = "#2E9FDF", # color para las variables
                col.ind = "#696969",  # color para las observaciones
                geom = "point"
                )
```

#### Resultados por Variables

Empezamos por las contribuciones de las variables a las primeras dos componentes principales.

```{r, echo=FALSE}
res.var=get_pca_var(datos.acp)
round(res.var$contrib[,c(1,2)], 4)      # Contribuciones a las CP
```

Tambien apreciamos la calidad de representación de las variables a las componentes principales.

```{r, echo=FALSE}
round(res.var$cos2[,c(1,2)], 5)
```

Hemos podido comprobar lo expuesto anteriormente:

- Se aprecia una important contribución de la variable `Bilirubin` a la primera componente principal, además de una muy buena representación (un valor de cos2 muy elevado).

- Seguidamente, la variable `Platelets`, `Age` y `Cholesterol` tiene gran contribución a la segunda componente principal.

- Observemos que la variable `Cholesterol`, a pesar de ser la tercera variable mejor representada por la segunda componente principal, es la que mejor (de entre estas tres) está representa en conjunto con la CP1 y la CP2.

# Cluster + ACP

Pintemos ahora el mismo gráfico, pero coloreadolo en función de los clusters que se nos habían generado en *k-means*:

```{r, echo=FALSE}
Cluster_4means = km_clusters_4$cluster %>% as.factor()

datos.acp$clusters = Cluster_4means



fviz_pca_biplot(datos.acp,  repel = TRUE,
                col.var = "grey", # color para las variables
                col.ind = Cluster_4means,  # color para las observaciones
                geom = "point"
                )

```

Viendo este gráfico parece que clusteriza en función de la contribución de cada muestra a las componentes principales. Como la variable `Bilirubin` está bien representada por la primera componente principal, es normal que un cluster (círculos rojos) sean las muestras que más aportan a ésta.

Luego están las muestras azules que corresponden a aquellas que parece que estan representadas por `Cholesterol` y `Platelets`. 

A continuación, encontramos las verdes que son las que estan más representadas por `Age`, que era una de las variables que mejor se representaba por la segunda componente principal.

Y finalmente, encontramos el grupo lila que son las que estan mas representadas por `Albumin` que notemos que también contribuye bastante en la primera componente principal

# Conclusiones

- Usando *k-means* es difícil establecer un número óptimo de clusters, ya que con el método `wss` era complicado establemcer un $k$ óptimo; con el método `silhouette` no decía que el $k$ óptimo era $2$; y el método `gap_stat` decía $k=8$.

- Usando *k-medoids*, el número óptimo de clusters es $2$ usando los tres métodos.

- Tanto *k-means* como *k-medoids* como *Clara*, con $4$ clusters no agrupa en función de la variable `stage`.

- Utilizando dendogramas, o clasificación jerárquica, observamos que las distancias *Single or Minimum* y *Average* preservan mejor las distancias entre observaciones, en cambio *Ward's minimum variance* en nustro caso no es recomendable usarla.

- En cuanto a ACP, observamos que la variable `Bilirubin` está bien representada por la primera componente principal. Seguidamente, la variable `Platelets`, `Age` y `Cholesterol` tiene gran contribución a la segunda componente principal.

- Los clusters que se han realizado con $k=4$ parecen estar hechos en función de la calidad de representación de las variables que más bien estan representadas por las dos primeras componentes principales.
