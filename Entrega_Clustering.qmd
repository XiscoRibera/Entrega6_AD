---
title: "Clustering"
author: "Xisco Ribera & Irene Julià"
format:
  html:
    toc: true
    toc-depth: 5
editor: visual
---

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(dplyr)
library(GGally)
library(psych)
library(skimr)
library(fmsb)
library(mvnormtest)
library(car)
library(nortest)
library(fBasics)
library(ggplot2)
library(factoextra)
library(stats)
library(cluster)
```

# Recordatorio

Vamos a tratar una base de datos relacionados con la predicción de riesgo de cirrosis. La cirrosis es una etapa tardía de la cicatrización (fibrosis) del hígado causada por muchas formas de enfermedades y afecciones hepáticas, como la hepatitis y el alcoholismo crónico.

Nuestro objetivo para este estudio sería estudiar algunos perfiles con riesgo de padecer cirrosis.

Recordemos nuestra tabla de datos:

```{r}
datos <- read.table("cirrosis_tidy.csv",  header = TRUE )
datos = datos %>% mutate(Ascites = Ascites %>% as.factor,
                         Hepatomegaly = Hepatomegaly %>% as.factor,
                         Spiders = Spiders %>% as.factor,
                         Edema = Edema %>% as.factor,
                         Stage = Stage %>% as.factor,
                         Drug = Drug %>% as.factor,
                         Sex = Sex %>% as.factor,
                         Status = Status %>% as.factor)


glimpse(datos)
```

El tibble resultante consta de 276 observaciones y 20 variables. Cada muestra representa un paciente al que se le ha extraido la siguiente información:

-   `ID`: Identificador único
-   `N_Days`: Número de días entre el registro y la fecha de defunción, transplante o estudio analítico enJulio de 1986.
-   `Status`: Estatus del paciente: C (Censurado), CL (censurado debido a tratamiento hepático), o D (Muerto)
-   `Drug`: Tipo de fármaco: D-penicilamina o placebo
-   `Age`: Edad \[días\]
-   `Sex`: Sexo cromosómico: Male (hombre) o Female (Mujer)
-   `Ascites`: Presencia de Ascitis No o Si
-   `Hepatomegaly`: Presencia de Hepatomegalia No o Si
-   `Spiders`: Presencia de arañas vasculares No o Si
-   `Edema`: Presencia de Edema: No (no hay edema y sin tratamiento diurético para el edema), Sin (presencia de edema sin diuréticos, o edema curado con diuréticos), o Si (edema a pesar del tratamiento con diuréticos)
-   `Bilirubin`: Bilirrubina sérica \[mg/dl\]
-   `Cholesterol`: Colesterol sérico \[mg/dl\]
-   `Albumin`: Albúmina \[g/dl\]
-   `Copper`: Cobre en orina \[ug/day\]
-   `Alk_Phos`: Fosfatasa alcalina \[U/liter\]
-   `SGOT`: SGOT \[U/ml\]
-   `Triglycerides`: Triglicéridos \[mg/dl\]
-   `Platelets`: Plaquetas por cúbico \[ml/1000\]
-   `Prothrombin`: Tiempo de Protrombina \[s\]
-   `Stage`: Estado histórico de la enfermedad (1, 2, 3, or 4)

## Resumen numérico de las variables

-   Datos cuantitativos:

```{r, echo=FALSE}
# Separamos los datos en variables cuantitativas y cualitativas
datos_quant <- datos %>% 
  select(where(is.numeric)) %>% 
  select(-1)


datos_qual <- datos %>% 
  select(where(is.factor))


# Cuantitativas

Unidad = c("Días", "Días", "mg/dl", "mg/dl", "g/dl", "ug/día", "U/l", "U/ml", "mg/dl", "ml/1000", "s")

Media = round(colMeans(datos_quant),3)

rango <- function(x){
  return(max(x)-min(x))
}
Rango = round(apply(datos_quant, FUN = rango, MARGIN = 2), 3)
Minimo = round(apply(datos_quant, FUN = min, 2),3)
Maximo = round(apply(datos_quant, FUN = max, 2),3)
Desv = round(apply(datos_quant, FUN = sd, 2), 3)

tabla = data.frame(Unidad, Media, Desv, Minimo, Maximo, Rango)

tabla
```

-   Datos cualitativos:

```{r, echo=FALSE}
summary(datos_qual)
```

## Análisis de normalidad multivariante

Con estos datos vamos a realizar nuestro estudio de normalidad multivariante.

Calculemos el vector de medias

```{r, echo = FALSE}
Medias = colMeans(datos_quant) # vector de medias

S = cov(datos_quant) # matriz de covarianza
```

y la distancia de Mahalanobis:

```{r}
d_Mahalanobis = apply(datos_quant, MARGIN = 1, function(x)
                    t(x - Medias)%*%solve(S)%*%(x - Medias))
```

Una vez calculadas estas medidas, representemos los datos

```{r, echo=FALSE}
plot(qchisq((1:nrow(datos_quant) - 1/2) / nrow(datos_quant), df = 3), sort(d_Mahalanobis), xlab = expression(paste("Cuantiles de la ", chi[20]^2)),ylab="Distancias ordenadas")
abline(a=0,b=1)
```

Notemos que no sigue una Chi-cuadrado, i por tanto los datos tampoco siguen una normal multivariante.

Vamos a realizar un test de normalidad para confirmarlo. Utilizaremos Shapiro-Wilk:

```{r}
mvnormtest::mshapiro.test(t(datos_quant))
```

Obtenemos un p-valor muy pequeño, prácticamente 0, entonces, rechazamos la hipótesis nula y concluimos que no hay normalidad multivariante, es decir, almenos una variable individual no se distribuye normalmente.

# Clustering

En primer lugar vamos a tipificar o escalar nuestros datos para que esten todos a la misma escala:

```{r}
datos2 <- datos_quant %>% scale()
```

A continuación realicemos una representación gráfica de matrices de distancia:

Primero de todo vamos a centrar la matriz de datos:

```{r}
n <- dim(datos2)[1]
X <- as.matrix(datos2)
Hn <- diag(n)-1/n # matriz de centrado
cX <- Hn%*%X # matriz centrada
```

```{r}
mat_dist <- dist(x = cX, method = "euclidean")
```

```{r, cache = TRUE}
fviz_dist(dist.obj = mat_dist, lab_size = 5) +
 theme(legend.position = "none")
```

## K-means

En nuestro caso, no sabemos cuantos clusters o grupos esta dividido nuestro dataset. Por tanto vamos a estimar al número $k$ óptimo para aplicar el `kmeans()`. Para ello utilizaremos la función `fviz_nbclust()`:

```{r}
fviz_nbclust(x = cX, FUNcluster = kmeans, method = "wss",
 diss = dist(cX, method = "euclidean")) +
 geom_vline(xintercept = 3, linetype = 2)

```


```{r}
#set.seed(2312)
km_clusters <- kmeans(x = cX, centers = 3, nstart = 78)

fviz_cluster(object = km_clusters, data = cX, show.clust.cent = TRUE, geom ="point",
 ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
theme_bw() +
theme(legend.position = "none")
```

Como nuestro número de variables (dimensionalidad) es mayor de 2, automáticamente realiza un PCA y representa las dos primeras componentes principales (Dim1 y Dim2)

```{r}
km_clusters
```

## K-medoids (PAM)

En este caso, cada cluster está representado por una observación presente en el cluster (medoid), mientras que en K-means cada cluster está representado por su centroide, que se corresponde con el promedio de todas las observaciones del cluster pero con ninguna en particular.

```{r, warning=FALSE}
fviz_nbclust(x = cX, FUNcluster = pam, method = "wss",
 diss = dist(datos, method = "euclidean")) +
  geom_vline(xintercept = 6, linetype = 2)
```

```{r}
pam_clusters <- pam(x = cX, k = 3, metric = "euclidean")

fviz_cluster(object = pam_clusters, data = cX, ellipse.type = "t", geom = "point",  repel = TRUE) +
  theme_bw() + 
  theme(legend.position = "none")
```

## Dendograma

```{r, cache =TRUE}
set.seed(101)
hc_completo <- datos_quant %>% scale() %>% dist(method = "euclidean") %>%
 hclust(method = "complete")
fviz_dend(x = hc_completo, k = 3, cex = 0.6) +
 geom_hline(yintercept = 10.5, linetype = "dashed")
```



## Análisis de Componentes Principales

Calculamos las componentes principales con el comando `prcomp` utilizando nuestro dataset, sin escalar los datos ya que están todos en la misma escala. Por último, los consideramos centrados en el 0.

```{r}
datos.acp=prcomp(cX, scale = FALSE, center = FALSE)
```

Los valores propios muestran el porcentaje de varianza explicada por cada componente principal.

```{r, echo=FALSE}
lambdas = get_eigenvalue(datos.acp)
round(lambdas, 5)
```

Notemos que usando las 2 primeras componentes principales explican aproximadamente el 93.3% de los datos. Veamos el gráfico del codo:

```{r, echo=FALSE}
fviz_eig(datos.acp, addlabels = TRUE, ylim=c(0,100))
```

Efectivamente, viendo el gráfico y utilizando el criterio del codo, nos quedaremos con las dos primeras componentes principales.

Realicemos un gráfico de círculo de correlación variable para ver como se agrupan las variables y la calidad de representación que tienen.

```{r, echo=FALSE}
fviz_pca_var(datos.acp, axes = c(1,2), col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)
```



Para corroborar numéricamente la calidad de representación, realizaremos un gráfico de cos2.

```{r, echo=FALSE}
var <- get_pca_var(datos.acp)
fviz_cos2(datos.acp, choice = "var", axes = 1:2)
```

Efectivamente, las tres primeras variables tienen un cos2 alto, por tanto estas bien representadas. En cambio, las otras variables tienen un valor de cos2 prácticamente nulo, indica que estas variables no estan bien representadas por las componentes principales.

Para ver como se relacionan las componentes principales con los datos originales, veamos los autovectores.

```{r, echo=FALSE}
round(datos.acp$rotation,4)
```

Observemos que la primera componente principal da un peso muy elevado y positivo a la variable `Agrucultura`, y de signo contrario a las demás (excepto `Mineria` con un peso muy bajo).

Respecto a la segunda componente principal, representa sobretodo a `Fábrica` y a `Servicios Sociales y Personales` (con signos opuestos pero un peso elevado).

Ahora, vamos a realizar un `biplot`, que nos permitirá visualizar las variables originales y las observaciones transformadas en los ejes de componentes principales.

```{r, echo=FALSE}
fviz_pca_biplot(datos.acp,  repel = TRUE,
                col.var = "#2E9FDF", # color para las variables
                col.ind = "#696969"  # color para las observaciones
                )
```

De nuevo, se aprecia como la variable `Agricultura` es la mejor representada debido a la longitud de la flecha. Le siguen las variables de `Fábricas` y `Servicios Sociales y Personales`, mejor representadas por la segunda componente.

### Resultados del Análisis

Llegado a este punto, vamos a comprobar numéricamente todas las conclusiones que hemos sacado anteriormente. Realmente basta restringirnos a las dos primeras componentes principales ya que en el estudio hemos decidido utilizar solamente estas dos.

#### Resultados por Variables

Empezamos por las contribuciones de las variables a las componentes principales.

```{r, echo=FALSE}
res.var=get_pca_var(datos.acp)
round(res.var$contrib, 4)      # Contribuciones a las CP
```

Tambien apreciamos la calidad de representación de las variables a las componentes principales.

```{r, echo=FALSE}
round(res.var$cos2, 5)
```

Respecto a las variables, hemos podido comprobar lo expuesto anteriormente, se aprecia una fuerte contribución de la variable `Agricultura` a la primera componente principal, además de una muy buena representación (un valor de cos2 muy elevado).

Seguidamente, la variable `Fábricas` tiene gran contribución a la segunda componente principal, con menor representación debido al valor de cos2 y que también contribuye a la primera componente principal, pero muy poco. En cambio, la variable `Servicios sociales y Personales` tiene más poca contribución a la segunda componente, pero está algo mejor representada por la primera que la variable anterior; de todos modos, la calidad de representación es más baja que las anteriores.

No podemos destacar más variables ya que, como vimos en los gráficos, no habia prácticamente representación por parte de las componentes principales.

#### Resultados por Observaciones

Ahora, respecto a las observaciones, empezamos por las coordenadas.

```{r, echo=FALSE}
res.obs=get_pca_ind(datos.acp)
round(res.obs$coord, 2)  #Coordenadas
```

También las contribuciones de cada observación a las componentes principales.

```{r, echo=FALSE}
round(res.obs$contrib,2)  #Contribuciones a las CP
```

Por último, la calidad de representación, es decir, el valor de cos2.

```{r, echo=FALSE}
round(res.obs$cos2,3)  # Calidad de la representación
```

### Grupos

Vamos a tratar de determinar grupos de países con comportamientos semejantes en la distribución de su fuerza de trabajo.

El criterio que seguiremos es separar las observaciones dependiendo de su contribucón a las componentes principales. Como la variable `Agricultura` es la que tiene una mayor representación, uno de los grupos estará formado por los paises cuya mayor fuerza de trabajo se basa en la agricultura. Para determinar este grupo, consideraremos los paises que tienen coordenada positiva en el eje de la primera componente principal.

Las otras dos variables mejor representadas son `Fábricas` y `Servicios Sociales y Personales`. La relación de estas variables con la primera componente principal es negativa, así que consideramos el resto de paises para clasificar en estos dos grupos. Esta clasificación se va a determinar mediante la segunda componente principal: la relación de `Fábricas` con la componente principal es positiva, mientras que con la otra variable es negativa. Por tanto, utilizaremos estas condiciones para dividir los paises.

```{r, echo=FALSE}
coord = data.frame(res.obs$coord[, c(1,2)])
coord = coord %>% 
  mutate(Grupos = if_else(Dim.1 > 0, "Grupo 1", if_else(Dim.2 > 0, "Grupo 2", "Grupo 3")))
coord
```

Añadimos este factor a nuestra tabla de datos original y realizamos un gráfico para visualizar la división de los grupos.

```{r, echo=FALSE}
datos = datos %>% 
  mutate(Grupo = as.factor(coord$Grupos))
glimpse(datos)
```

```{r, echo=FALSE}
autoplot(datos.acp, data = datos, colour = 'Grupo',
         loadings = TRUE, loadings.colour = 'blue',
         loadings.label = FALSE, loadings.label.size = 3, size = 3) +
  geom_text_repel(aes(label = rownames(datos)), box.padding = 0.5, segment.color = "darkgrey", size = 3, color = "darkgrey") +
  theme_bw()
```

Veamos en un mapa de Europa con los paises pintados en función del grupo al cual le hemos asignado:

![](Europa%20Map%20Chart.png)

Nota: Como los datos se tomaron en 1979, el mapa de Europa ha cambiado respecto al actual. Algunos paises se han fragmentado y otros se han unido. En concreto:

-   Debido a la separación de Alemania por muro de Berlín después de la Seguna Guerra Mundial se diferenció entre Alemania Oriental y Alemania Occidental (hasta 1989, que se volvió a unir en un solo estado). Por tanto, en el mapa solo hemos considerado Alemania.

-   Hay una muestra llamada "Checoslovaquia", que actualmente este estado esta separado en Republica Checa y Eslovaquia (separación en 1992).

-   Del mismo modo aparece la Unión Sovietica, cuyos paises que la conformaban son: Rusia, Transcaucásicas (Armenia, Azerbayán y Georgia), Ucrania y Bielorrusia (disolución en 1991).

-   Por último tenemos Yugoslavia: Bosnia y Herzegovina, Croacia, Eslovenia, Macedonia, Montenegro y Serbia (disolución en 1992).

A la hora de pintar el mapa, todos los paises actuales que formaban parte de algun estado mencionado anteriormente se han coloreado del mismo color que el estado al que pertenecían. Por tanto, las conclusiones que se extraen no son actuales ya que cada país ahora es independiente.

### Conclusión

-   Usando el método de componentes principales hemos podido reducir el número de variables a 2, ya que explican aproximadamente el 93.3% de los datos.

-   Las variables iniciales que mejor se han podido describir con las dos componentes principales han sido `Agricultura`, `Fábrica` y `Servicios Sociales y Personales`.

-   Hemos podido establecer 3 grupos de paises con comportamientos semejantes en función de si su fuerza de trabajo se distribuía más en `Agricultura` (Grupo 1), `Fábrica` (Grupo 2) y `Servicios Sociales y Personales` (Grupo 3).
















