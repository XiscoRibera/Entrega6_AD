---
title: "Clustering"
author: "Xisco Ribera & Irene Julià"
format:
  html:
    toc: true
    toc-depth: 5
editor: visual
---

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(dplyr)
library(GGally)
library(psych)
library(skimr)
library(fmsb)
library(mvnormtest)
library(car)
library(nortest)
library(fBasics)
library(ggplot2)
library(factoextra)
library(stats)
library(cluster)
library(NbClust)
library(ggrepel)
```

# Recordatorio

Vamos a tratar una base de datos relacionados con la predicción de riesgo de cirrosis. La cirrosis es una etapa tardía de la cicatrización (fibrosis) del hígado causada por muchas formas de enfermedades y afecciones hepáticas, como la hepatitis y el alcoholismo crónico.

Nuestro objetivo para este estudio sería estudiar algunos perfiles con riesgo de padecer cirrosis.

Los datos provienen de la página web Kaggle: [Kaggle - Cirrhosis](https://www.kaggle.com/datasets/fedesoriano/cirrhosis-prediction-dataset/).

Recordemos nuestra tabla de datos:

```{r}
datos <- read.table("cirrosis_tidy.csv",  header = TRUE )
datos = datos %>% mutate(Ascites = Ascites %>% as.factor,
                         Hepatomegaly = Hepatomegaly %>% as.factor,
                         Spiders = Spiders %>% as.factor,
                         Edema = Edema %>% as.factor,
                         Stage = Stage %>% as.factor,
                         Drug = Drug %>% as.factor,
                         Sex = Sex %>% as.factor,
                         Status = Status %>% as.factor)


glimpse(datos)
```

El tibble resultante consta de 276 observaciones y 20 variables. Cada muestra representa un paciente al que se le ha extraido la siguiente información:

-   `ID`: Identificador único
-   `N_Days`: Número de días entre el registro y la fecha de defunción, transplante o estudio analítico en Julio de 1986.
-   `Status`: Estatus del paciente: C (Censurado), CL (censurado debido a tratamiento hepático), o D (Muerto)
-   `Drug`: Tipo de fármaco: D-penicilamina o placebo
-   `Age`: Edad \[días\]
-   `Sex`: Sexo cromosómico: Male (hombre) o Female (Mujer)
-   `Ascites`: Presencia de Ascitis: No o Si
-   `Hepatomegaly`: Presencia de Hepatomegalia: No o Si
-   `Spiders`: Presencia de arañas vasculares: No o Si
-   `Edema`: Presencia de Edema: No (no hay edema y sin tratamiento diurético para el edema), Sin (presencia de edema sin diuréticos, o edema curado con diuréticos), o Si (edema a pesar del tratamiento con diuréticos)
-   `Bilirubin`: Bilirrubina sérica \[mg/dl\]
-   `Cholesterol`: Colesterol sérico \[mg/dl\]
-   `Albumin`: Albúmina \[g/dl\]
-   `Copper`: Cobre en orina \[ug/day\]
-   `Alk_Phos`: Fosfatasa alcalina \[U/liter\]
-   `SGOT`: SGOT \[U/ml\]
-   `Triglycerides`: Triglicéridos \[mg/dl\]
-   `Platelets`: Plaquetas por cúbico \[ml/1000\]
-   `Prothrombin`: Tiempo de Protrombina \[s\]
-   `Stage`: Estado histórico de la enfermedad (1, 2, 3, or 4)

## Resumen numérico de las variables

-   Datos cuantitativos:

```{r, echo=FALSE}
# Separamos los datos en variables cuantitativas y cualitativas
datos_quant <- datos %>% 
  select(where(is.numeric)) %>% 
  select(-1)


datos_qual <- datos %>% 
  select(where(is.factor))


# Cuantitativas

Unidad = c("Días", "Días", "mg/dl", "mg/dl", "g/dl", "ug/día", "U/l", "U/ml", "mg/dl", "ml/1000", "s")

Media = round(colMeans(datos_quant),3)

rango <- function(x){
  return(max(x)-min(x))
}
Rango = round(apply(datos_quant, FUN = rango, MARGIN = 2), 3)
Minimo = round(apply(datos_quant, FUN = min, 2),3)
Maximo = round(apply(datos_quant, FUN = max, 2),3)
Desv = round(apply(datos_quant, FUN = sd, 2), 3)

tabla = data.frame(Unidad, Media, Desv, Minimo, Maximo, Rango)

tabla
```

-   Datos cualitativos:

```{r, echo=FALSE}
summary(datos_qual)
```

## Análisis de normalidad multivariante

Con estos datos vamos a realizar nuestro estudio de normalidad multivariante.

Calculemos el vector de medias

```{r, echo = FALSE}
Medias = colMeans(datos_quant) # vector de medias

S = cov(datos_quant) # matriz de covarianza
```

y la distancia de Mahalanobis:

```{r}
d_Mahalanobis = apply(datos_quant, MARGIN = 1, function(x)
                    t(x - Medias)%*%solve(S)%*%(x - Medias))
```

Una vez calculadas estas medidas, representemos los datos

```{r, echo=FALSE}
plot(qchisq((1:nrow(datos_quant) - 1/2) / nrow(datos_quant), df = 3), sort(d_Mahalanobis), xlab = expression(paste("Cuantiles de la ", chi[20]^2)),ylab="Distancias ordenadas")
abline(a=0,b=1)
```

Notemos que no sigue una Chi-cuadrado, i por tanto los datos tampoco siguen una normal multivariante.

Vamos a realizar un test de normalidad para confirmarlo. Utilizaremos Shapiro-Wilk:

```{r}
mvnormtest::mshapiro.test(t(datos_quant))
```

Obtenemos un p-valor muy pequeño, prácticamente 0, entonces, rechazamos la hipótesis nula y concluimos que no hay normalidad multivariante, es decir, almenos una variable individual no se distribuye normalmente.

# Clustering

Vamos a guardar en un nuevo dataset las variables cuantitativas. Vamos a tipificar o escalar nuestros datos para que esten todos a la misma escala:

```{r}
datos2 <- datos_quant %>% scale()
```

A continuación, realicemos una representación gráfica de matrices de distancia:

Primero de todo vamos a centrar la matriz de datos:

```{r}
n <- dim(datos2)[1]
X <- as.matrix(datos2)
Hn <- diag(n)-1/n # matriz de centrado
cX <- Hn%*%X # matriz centrada
```

```{r}
mat_dist <- dist(x = cX, method = "euclidean")
```

```{r, cache = TRUE}
fviz_dist(dist.obj = mat_dist, lab_size = 5) +
 theme(legend.position = "none")
```

## K-means

En nuestro caso, no sabemos en cuantos clusters o grupos esta dividido nuestro dataset. Por tanto, vamos a estimar al número $k$ óptimo para aplicar el método de `kmeans()`. Para ello, utilizaremos la función `fviz_nbclust()`:

```{r}
fviz_nbclust(x = cX, FUNcluster = kmeans, method = "wss",
 diss = dist(cX, method = "euclidean"))
```

Realmente, con el método del codo, es un poco complicado establemcer un $k$ óptimo, así que vamos a utilizar otros métodos que nos proporciona `R`:

```{r}
fviz_nbclust(x = cX, FUNcluster = kmeans, method = "silhouette", 
 diss = dist(cX, method = "euclidean"))
```

```{r}
fviz_nbclust(x = cX, FUNcluster = kmeans, method = "gap_stat", 
 diss = dist(cX, method = "euclidean"))
```

Con el método de la silueta, nos sugiere escoger $k=2$, en cambio, con el siguiente, nos sugiere $8$ clusters. Como no hemos salido de dudas, realicemos clusters para $k =2,4,8$ 



```{r, cache=TRUE, eval=FALSE, echo=FALSE}
# vamos a realizar otro experimento: vamos a realizar una función que nos proporciona 30 índices para determinar el número de clusters.
resnumclust = NbClust(data = cX, distance = "euclidean", min.nc = 2, max.nc = 10, 
                      method = "kmeans", index = "alllong")

# fix the functions
fviz_nbclust <- function (x, FUNcluster = NULL, method = c("silhouette", "wss", 
                                           "gap_stat"), diss = NULL, k.max = 10, nboot = 100, verbose = interactive(), 
          barfill = "steelblue", barcolor = "steelblue", linecolor = "steelblue", 
          print.summary = TRUE, ...) 
{
  set.seed(123)
  if (k.max < 2) 
    stop("k.max must bet > = 2")
  method = match.arg(method)
  if (!inherits(x, c("data.frame", "matrix")) & !("Best.nc" %in% 
                                                  names(x))) 
    stop("x should be an object of class matrix/data.frame or ", 
         "an object created by the function NbClust() [NbClust package].")
  if (inherits(x, "list") & "Best.nc" %in% names(x)) {
    best_nc <- x$Best.nc
    if (any(class(best_nc) == "numeric") ) 
      print(best_nc)
    else if (any(class(best_nc) == "matrix") )
      .viz_NbClust(x, print.summary, barfill, barcolor)
  }
  else if (is.null(FUNcluster)) 
    stop("The argument FUNcluster is required. ", "Possible values are kmeans, pam, hcut, clara, ...")
  else if (!is.function(FUNcluster)) {
    stop("The argument FUNcluster should be a function. ", 
         "Check if you're not overriding the specified function name somewhere.")
  }
  else if (method %in% c("silhouette", "wss")) {
    if (is.data.frame(x)) 
      x <- as.matrix(x)
    if (is.null(diss)) 
      diss <- stats::dist(x)
    v <- rep(0, k.max)
    if (method == "silhouette") {
      for (i in 2:k.max) {
        clust <- FUNcluster(x, i, ...)
        v[i] <- .get_ave_sil_width(diss, clust$cluster)
      }
    }
    else if (method == "wss") {
      for (i in 1:k.max) {
        clust <- FUNcluster(x, i, ...)
        v[i] <- .get_withinSS(diss, clust$cluster)
      }
    }
    df <- data.frame(clusters = as.factor(1:k.max), y = v, 
                     stringsAsFactors = TRUE)
    ylab <- "Total Within Sum of Square"
    if (method == "silhouette") 
      ylab <- "Average silhouette width"
    p <- ggpubr::ggline(df, x = "clusters", y = "y", group = 1, 
                        color = linecolor, ylab = ylab, xlab = "Number of clusters k", 
                        main = "Optimal number of clusters")
    if (method == "silhouette") 
      p <- p + geom_vline(xintercept = which.max(v), linetype = 2, 
                          color = linecolor)
    return(p)
  }
  else if (method == "gap_stat") {
    extra_args <- list(...)
    gap_stat <- cluster::clusGap(x, FUNcluster, K.max = k.max, 
                                 B = nboot, verbose = verbose, ...)
    if (!is.null(extra_args$maxSE)) 
      maxSE <- extra_args$maxSE
    else maxSE <- list(method = "firstSEmax", SE.factor = 1)
    p <- fviz_gap_stat(gap_stat, linecolor = linecolor, 
                       maxSE = maxSE)
    return(p)
  }
}

.viz_NbClust <- function (x, print.summary = TRUE, barfill = "steelblue", 
          barcolor = "steelblue") 
{
  best_nc <- x$Best.nc
  if (any(class(best_nc) == "numeric") )
    print(best_nc)
  else if (any(class(best_nc) == "matrix") ) {
    best_nc <- as.data.frame(t(best_nc), stringsAsFactors = TRUE)
    best_nc$Number_clusters <- as.factor(best_nc$Number_clusters)
    if (print.summary) {
      ss <- summary(best_nc$Number_clusters)
      cat("* Among all indices: \n===================\n")
      for (i in 1:length(ss)) {
        cat("*", ss[i], "proposed ", names(ss)[i], 
            "as the best number of clusters\n")
      }
      cat("\n \t *****Conclusion*****\n\n")
      cat("* According to the majority rule, the best number of clusters is ", 
          names(which.max(ss)), ".\n\n")
    }
    df <- data.frame(Number_clusters = names(ss), freq = ss, 
                     stringsAsFactors = TRUE)
    p <- ggpubr::ggbarplot(df, x = "Number_clusters", 
                           y = "freq", fill = barfill, color = barcolor) + 
      labs(x = "Number of clusters k", y = "Frequency among all indices", 
           title = paste0("Optimal number of clusters - k = ", 
                          names(which.max(ss))))
    return(p)
  }
}
# assign them to the factoextra namespace
environment(fviz_nbclust) <- asNamespace("factoextra")
assignInNamespace("fviz_nbclust",fviz_nbclust,"factoextra")
environment(.viz_NbClust) <- asNamespace("factoextra")
assignInNamespace(".viz_NbClust",.viz_NbClust,"factoextra")



fviz_nbclust(resnumclust)

# Vamos a calcular las 4 k-medias con nuestros datos y con 30 iteraciones. Además, vamos a visualizar como ha quedado la partición.
```

- 2 clusters:

```{r}
set.seed(232)
km_clusters_2 <- kmeans(x = cX, centers = 2, nstart = 70)
# km_clusters$cluster
```

Ahora bien, representemos dichos clusters en el plano. Como nuestro número de variables (dimensionalidad) es mayor de 2, automáticamente realiza un PCA y representa las dos primeras componentes principales (Dim1 y Dim2).

```{r}
fviz_cluster(object = km_clusters_2, data = cX, show.clust.cent = TRUE, geom ="point",
 ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, palette = c("#FF7078", "#A298E8")) +
theme_bw() +
theme(legend.position = "none")
```

- 4 clusters:


```{r}
set.seed(232)
km_clusters_4 <- kmeans(x = cX, centers = 4, nstart = 70)

fviz_cluster(object = km_clusters_4, data = cX, show.clust.cent = TRUE, geom ="point",
 ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, palette = c("#FF7078", "#F39B4C", "#7FBFF5", "#A298E8")) +
theme_bw() +
theme(legend.position = "none")
```

- 8 clusters:


```{r}
set.seed(232)
km_clusters_8 <- kmeans(x = cX, centers = 8, nstart = 70)

fviz_cluster(object = km_clusters_8, data = cX, show.clust.cent = TRUE, geom ="point",
 ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, palette = c("#FF7078", "#FF7F00", "#7FBFF5", "#8B3A62","#FFB90F", "#1C86EE",  "#FF3030", "#00CD00")) +
theme_bw() +
theme(legend.position = "none")
```


Como podemos ver, al menos en la proyección en 2 dimensiones, hay bastante solapamiento. Además, si vemos la zona pintada como si fuera un intervalo de confianza, hay muchas observaciones que quedan fuera.


Ahora bien, sería interesante ver si el caso de k = 4, los clusters corresponden a las fases de cirrosis segun la variable `stage`.

Vamos a crear un data frame compuesto de 3 columnas: identificador del paciente, estado de la enfermedad y cluster al que pertenece.

```{r}
id_stage = datos %>% 
  select(ID, Stage)
Cluster = km_clusters_4$cluster %>% as.factor()

tabla_cluster = cbind(id_stage, Cluster)

table(x = tabla_cluster[,c(2,3)])
```

Observemos que en este caso un kmeans con $k=4$ no clusteriza en función del estado de cirrosis en que se encuentra el paciente.

## K-medoids (PAM)

En este caso, cada cluster está representado por una observación presente en el cluster (medoid), mientras que en K-means cada cluster está representado por su centroide, que se corresponde con el promedio de todas las observaciones del cluster pero con ninguna en particular. Vamos a utilizar la distancia de Manhattaan ya que es menos sensible a outliers.

```{r, warning=FALSE}
fviz_nbclust(x = cX, FUNcluster = pam, method = "wss",
 diss = dist(datos, method = "manhattan"))
```

Encontramos codos en $k=2$, $k=4$ y uno muy pronunciado en $k=6$. Para ver cual sería óptimo, como en el caso anterior, visualicemos otros métodos como el de la silueta i gap_stat.

```{r, warning=FALSE}
fviz_nbclust(x = cX, FUNcluster = pam, method = "silhouette",
 diss = dist(datos, method = "manhattan"))
```

```{r, warning=FALSE}
fviz_nbclust(x = cX, FUNcluster = pam, method = "gap_stat",
 diss = dist(datos, method = "manhattan"))
```

Estos dos métodos nos sugieren 2 clusters para realizar el análisis.


```{r, cache=TRUE,, eval=FALSE, echo=FALSE}
# Como en `kmeans`, podemos utilizar la misma función para corroborarlo con 30 índices, utilizando el método `median`, que es el más indicado para medoides:

resnumclust2 = NbClust(data = cX, distance = "euclidean", min.nc = 2, max.nc = 10, 
                      method = "median", index = "alllong")
```

```{r, cache=TRUE, eval=FALSE, echo=FALSE}
fviz_nbclust(resnumclust2)

# Esta función nos sugiere escoger 2 clusters. De este modo, calculamos los clusters y los representamos
```


```{r}
pam_clusters <- pam(x = cX, k = 2, metric = "manhattan")

fviz_cluster(object = pam_clusters, data = cX, ellipse.type = "t", geom = "point", repel =TRUE) +
  theme_bw() + 
  theme(legend.position = "none")
```

Estan bastante solapados, puede deberse a la proyección a dos dimensiones.

Realicemos kmedoids con $k=4$ para comparar con la variable `stage`:

```{r}
pam_clusters_4 <- pam(x = cX, k = 4, metric = "manhattan")

fviz_cluster(object = pam_clusters_4, data = cX, ellipse.type = "t", geom = "point", repel =TRUE) +
  theme_bw() + 
  theme(legend.position = "none")
```
```{r}
id_stage = datos %>% 
  select(ID, Stage)

Cluster_medoid = pam_clusters_4$clustering %>% as.factor()

tabla_cluster = cbind(id_stage, Cluster_medoid)

table(x = tabla_cluster[,c(2,3)])
```

Observemos que en este caso un kmedoids con $k=4$ tampoco clusteriza en función del estado de cirrosis en que se encuentra el paciente.

## Clara



## Dendograma

Por útimo, realicemos un dendograma para ver como se van formando los clusters, con el algoritmo *agglomerative hierarchical clustering*. 

Como sabemos, existen diferentes tipos de dendogramas en función del tipo de distancia entre cluster que se escoja. Recordemos cuales son:

- **Complete or Maximum:** Se calcula la distancia entre todos los posibles pares formados por una observación del cluster A y una del cluster B. La mayor de todas ellas se selecciona como la distancia entre los dos clusters.

- **Single or Minimum:** Se calcula la distancia entre todos los posibles pares formados por una observación del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters.

- **Average:** Se calcula la distancia entre todos los posibles pares formados por una observación del cluster A y una del cluster B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters.

- **Centroid:** Se calcula el centroide de cada uno de los clusters y se selecciona la distancia entre ellos como la distancia entre los dos clusters.

- **Ward:** Se trata de un método general. La selección del par de clusters que se combinan en cada paso del *agglomerative hierarchical clustering* se basa en el valor óptimo de una función objetivo, pudiendo ser esta última cualquier función definida por el analista. Un caso particular es *Ward's minimum variance*, en cada paso se identifican aquellos 2 clusters cuya fusión conlleva menor incremento de la varianza total intra-cluster.


- Dendograma para 4 clusters con distancia entre clusters *Complete or Maximum*:

```{r, cache=TRUE, warning=FALSE}
hc_completo <- mat_dist %>%
  hclust(method = "complete") # Complete or Maximum

fviz_dend(x = hc_completo, k = 4, cex = 0.6) +
  geom_hline(yintercept = 11.2, linetype = "dashed") +
  geom_hline(yintercept = 9.6, linetype = "dashed")
```

- Dendograma para 4 clusters con distancia entre clusters *Single or Minimum*:


```{r, cache=TRUE, warning=FALSE}
hc_single <- mat_dist %>%
  hclust(method = "single") # Single or Minimum

fviz_dend(x = hc_single, k = 4, cex = 0.6) +
 geom_hline(yintercept = 5.9, linetype = "dashed")+
  geom_hline(yintercept = 5.1, linetype = "dashed")
```

- Dendograma para 4 clusters con distancia entre clusters *Average*:

```{r, cache=TRUE, warning=FALSE}
hc_average <- mat_dist %>%
  hclust(method = "average") # Average

fviz_dend(x = hc_average, k = 4, cex = 0.6) +
 geom_hline(yintercept = 8.8, linetype = "dashed")+
  geom_hline(yintercept = 7, linetype = "dashed")
```

- Dendograma para 4 clusters con distancia entre clusters *Ward's minimum variance*:

```{r, cache=TRUE, warning=FALSE}
hc_ward_D2 <- mat_dist %>%
  hclust(method = "ward.D2") # Ward's minimum variance

fviz_dend(x = hc_ward_D2, k = 4, cex = 0.6) +
 geom_hline(yintercept = 25, linetype = "dashed")+
  geom_hline(yintercept = 18, linetype = "dashed")
```

- Observemos que al tener muchas observaciones no podemos distinguirlas. Aun así nos podemos hacer una idea del tamaño de cada cluster. 

- Observamos que en nuestros datos los clusters que se forman dependen mucho de la distancia entre clusters que se escoja. Esto puede indicar que los datos no esten distribuidos en grupos bien diferenciados.

- Observamos que si cortamos más arriba el dendograma (la línea más alta), podemos ver cuales son los clusters que se juntan si quisiéramos solo 2 clusters.


Una vez calculados los dendogramas, hay que comprobar que la estructura preserva bien las distancias originales entre las observaciones. Para hacerlo usemos el coeficiente de correlación entre las distancias *cophenetic* del dendrograma y la matriz de distancias original. Cuanto más cercano a 1 sea el valor, mejor refleja el dendograma las distancias originales.


```{r}
cor(x = mat_dist, cophenetic(hc_completo))
cor(x = mat_dist, cophenetic(hc_single))
cor(x = mat_dist, cophenetic(hc_average))
cor(x = mat_dist, cophenetic(hc_ward_D2))
```

- Observamos que las distancias *Single or Minimum* y *Average* preservan mejor las distancias entre observaciones, en cambio *Ward's minimum variance* en nustro caso no es recomendable usarla.


# Análisis de Componentes Principales

Calculamos las componentes principales con el comando `prcomp` utilizando nuestro dataset, sin escalar los datos ya que están todos en la misma escala. Por último, los consideramos centrados en el 0.

```{r}
datos.acp=prcomp(datos_quant, scale = TRUE, center = TRUE)
```

Los valores propios muestran el porcentaje de varianza explicada por cada componente principal.

```{r, echo=FALSE}
lambdas = get_eigenvalue(datos.acp)
round(lambdas, 5)
```

```{r, echo=FALSE}
fviz_eig(datos.acp, addlabels = TRUE, ylim=c(0,100))
```

Efectivamente, viendo el gráfico y utilizando el criterio del codo, nos quedaremos con

Realicemos un gráfico de círculo de correlación variable para ver como se agrupan las variables y la calidad de representación que tienen.

```{r, echo=FALSE}
fviz_pca_var(datos.acp, axes = c(1,2), col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)
```

Para corroborar numéricamente la calidad de representación, realizaremos un gráfico de cos2.

```{r, echo=FALSE}
var <- get_pca_var(datos.acp)
fviz_cos2(datos.acp, choice = "var", axes = 1:2)
```

Efectivamente, las tres primeras variables tienen un cos2 alto, por tanto estas bien representadas. En cambio, las otras variables tienen un valor de cos2 prácticamente nulo, indica que estas variables no estan bien representadas por las componentes principales.

Para ver como se relacionan las componentes principales con los datos originales, veamos los autovectores.

```{r, echo=FALSE}
round(datos.acp$rotation,4)
```

Observemos que la primera componente principal da un peso muy elevado y negativo a la variable `Bilirubin`.

Respecto a la segunda componente principal, representa sobretodo a `Platelets`,  a `Age` y a `Cholesterol`.

Ahora, vamos a realizar un `biplot`, que nos permitirá visualizar las variables originales y las observaciones transformadas en los ejes de componentes principales.

```{r, echo=FALSE}
fviz_pca_biplot(datos.acp,  repel = TRUE,
                col.var = "#2E9FDF", # color para las variables
                col.ind = "#696969",  # color para las observaciones
                geom = "point"
                )
```


### Resultados del Análisis

Llegado a este punto, vamos a comprobar numéricamente todas las conclusiones que hemos sacado anteriormente. Realmente basta restringirnos a las dos primeras componentes principales ya que en el estudio hemos decidido utilizar solamente estas dos.

#### Resultados por Variables

Empezamos por las contribuciones de las variables a las componentes principales.

```{r, echo=FALSE}
res.var=get_pca_var(datos.acp)
round(res.var$contrib, 4)      # Contribuciones a las CP
```

Tambien apreciamos la calidad de representación de las variables a las componentes principales.

```{r, echo=FALSE}
round(res.var$cos2, 5)
```

Respecto a las variables, hemos podido comprobar lo expuesto anteriormente, se aprecia una important contribución de la variable `Bilirubin` a la primera componente principal, además de una muy buena representación (un valor de cos2 muy elevado).

Seguidamente, la variable `Platelets`, `Age` y `Cholesterol` tiene gran contribución a la segunda componente principal.

Observemos que a variable `Cholesterol` a pesar de ser la tercera variable mejor representada por la segunda componente principal es la que mejor (de entre estas tres) está representa en conjunto con la CP1 y la CP2.

# Conclusiones

